<!DOCTYPE html>
<html>

  <head>
	<meta name="generator" content="Hugo 0.91.2" />

  <title>
      
      Kuba&#39;s Journal
      
  </title>

</head>


  <body>

    

	
<div class="h-feed">

	
	
	  <div class="h-entry">
		
			<h1><a href="https://journal.mackuba.eu/2025/01/16/postgress-progress/">Postgress progress</a></h1>
		

		<a href="https://journal.mackuba.eu/2025/01/16/postgress-progress/" class="u-url"><time class="dt-published" datetime="2025-01-16 22:39:26 &#43;0200">Jan 16, 2025</time></a>

		<div class="e-content">
			 <p>I think I&rsquo;ve finally made some good progress with my Postgres database. Things are working pretty well now, and I still have a few fixes in the queue. It&rsquo;s not all finished yet, but I feel like I&rsquo;m coming out of the woods now.</p>
<p>For those out of the loop: I have a server which streams and saves all posts from the Bluesky firehose, and uses this data to run several feeds on Bluesky used by 1k+ people, plus generates daily post statistics, the directory of custom handles and things like that.</p>
<p>This all has been running on SQLite so far (200+ GB file now), and it has served me well far longer than I would ever have expected, but in the end I&rsquo;m hitting its limits. The main problem is lack of support for concurrent writes, which makes it unrealistic to e.g. split the work into multiple workers, which I&rsquo;m gonna have to do sooner or later.</p>
<p>I&rsquo;ve only really used MySQL before on servers, but everyone was telling me I need to use Postgres, so I did what any normal person would have done (right?): I&rsquo;ve set up two test servers, one on MySQL and one on Postgres, running from two modified branches of the code, and I made the SQLite-based production server proxy some requests to one or the other in order to test them on real traffic. A sort of database A/B testing ðŸ¤ª</p>
<p>It has been working like this for a few months now, and I&rsquo;ve been slowly tweaking things, mostly on the Postgres side. Since I got it to work at all, they&rsquo;ve both been working ok, but MySQL has generally been doing more write I/O doing the same things, and Postgres has been doing comparably more read I/O.</p>
<p>The key part are the Replies feeds (<a href="https://bsky.app/profile/did:plc:oio4hkxaop4ao4wz2pp3f4cr/feed/follows-replies">Follows &amp; Replies</a> and <a href="https://bsky.app/profile/did:plc:oio4hkxaop4ao4wz2pp3f4cr/feed/replies">Only Replies</a>, it&rsquo;s basically the same code). These two are kind of variants of the built-in Following feed, showing the connecting user their own chronological timeline, generated live, filtered in some way. These feeds work by fetching the requesting user&rsquo;s follows list, either from the AppView or from a local cache, and then making a query that&rsquo;s basically: <em>&ldquo;give me the most recent 100 posts from any of these 50/200/1000/5000 users&rdquo;</em>.</p>
<p>There are two relevant indexes in that table: one on <code>(repo, time)</code> (repo = user&rsquo;s DID), and one on just <code>(time)</code>. Roughly speaking, for those users who follow e.g. 80 or 200 people, it makes more sense to scan the <code>(repo, time)</code> index those 80-200 times and collect the 100 most recent posts from all of those found, and for those who follow e.g. 9000 (yes, that happens ðŸ˜›), it&rsquo;s faster to scan the single <code>(time)</code> index until you find 100 relevant posts. But I&rsquo;ve been struggling to make Postgres always use the right index.</p>
<p>At first it tried to use the <code>(time)</code> index way too much. I tried to find a way to force it to use a specific index, but I was told it&rsquo;s not possible (<a href="https://pg-hint-plan.readthedocs.io">it might be though!</a>). With the help of some people on Bluesky, I managed to rewrite the query to use the other index, but then it worked bad for those people with a lot of follows. I interpreted the EXPLAINs as the query planner not having a good enough info about the structure of the data (it estimated <code>n_distinct</code>, the total number of distinct values in a column, as much lower than the actual number of users in the table), so I started messing with the &ldquo;statistics&rdquo; settings. I ended up bumping up the statistics target to the maximum 10,000 (making ANALYZE take really long) and hardcoded <code>n_distinct</code> to 4.5M, which was closer to the total count. I also increased <code>shared_buffers</code> to 1 GB. This improved things somewhat, to the &ldquo;not great, not terrible&rdquo; level, but I was still not happy with the performance.</p>
<p>But eventually, I was able to improve things a lot over the last few weeks:</p>
<ul>
<li>
<p>First, I was re-reading one of the blog posts about the differences in how MySQL and Postgres organize data internally, and I had a bit of an epiphany: unlike MySQL, Postgres doesn&rsquo;t store the primary key (<code>id</code>) in secondary indexes, since they reference the physical location of the data instead. In MySQL I had a version of the query which first did a subquery that collected just post IDs, and then used the final 100 IDs to fetch posts, but this couldn&rsquo;t work here if the indexes don&rsquo;t include <code>id</code>! This was something unexpected for me. So I replaced the larger index with <code>(repo, time DESC, id)</code> and rewrote the query again with a subquery, and the EXPLAIN showed &ldquo;Index Only Scan&rdquo; now, meaning it could use just the index for the subquery.</p>
</li>
<li>
<p>Next, the Index Only Scan was showing something like: &ldquo;Heap Fetches: 31362&rdquo;. This apparently means that even though it tries to only use the index in that step, it still has to go fetch 30k post rows from the actual table. This is because as records are updated or deleted (in my case only really deleted), Postgres just marks them as deleted, but doesn&rsquo;t free up the space or update the index until a VACUUM is done. A separate structure called &ldquo;visibility map&rdquo; tracks which table pages have some outdated records in them, and if an index entry points to such page, it has to go check if that record is still there. So I started <a href="https://bsky.app/profile/did:plc:oio4hkxaop4ao4wz2pp3f4cr/post/3lfg76issfc2c">tracking the state</a> of this visibility map, and set up a cron job to do the VACUUM twice a day to keep it relatively &ldquo;clean&rdquo;. (I wish it was as easy to do with my apartmentâ€¦)</p>
</li>
<li>
<p>Now, it was working great for people with short lists, but still tried to use the <code>(repo, time, id)</code> index for most very long lists. The EXPLAIN showed that it was vastly overestimating the cost of using <code>(time)</code>. I came to the conclusion thatâ€¦ the hardcoded <code>n_distinct</code> was actually working against it now, even though it was technically more precise. In my practical use cases, what actually matters is not how many different users have at least one post anywhere in the table, but how many are <em>actively posting</em> - whose posts you can find in the last few hours or a day. And that&rsquo;s much less than 4.5M. So I cleared the hardcoded <code>n_distinct</code> and let it find its own estimate, and reset the statistics target to the default 100 - and this improved things quite a bit! It was now almost perfectly using <code>index_posts_on_repo_time_id</code> to around ~1200 followed accounts, and <code>index_posts_on_time</code> above that.</p>
</li>
<li>
<p>Not related to the query/Postgres directly, but I also added a &ldquo;prefetching&rdquo; task to the follows list cache: it runs every 15 minutes and checks which follows lists are approaching the point where the cached data becomes stale (but was accessed recently) and reloads the list from the AppView in advance. This saves a couple to several seconds of time every time the list had to be refreshed before returning the feed, so it made quite a lot of difference.</p>
</li>
<li>
<p>One last thing I need to fix is that for long lists, significant time is spent on query planning (e.g. 1724 ms planning time and then 12 ms execution time). I&rsquo;m gonna try to restructure the query to pass the list of user IDs differently somehow and see if that helps, or perhaps I could move the cache to a new database table instead of keeping it in .json files.</p>
</li>
<li>
<p>And finally, I have some more config tweaking to do - e.g. it seems <code>shared_buffers</code> could be bumped up to at least 2 GB here (with 8 GB total RAM) - but I wanted to first make sure it&rsquo;s doing the right things before I give it more space to cache it.</p>
</li>
</ul>
<p>I&rsquo;m really looking forward to when I can finally set up a production instance with Postgres and move everything there and drop the other branches - but I feel like this moment is getting close ðŸ˜…</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://journal.mackuba.eu/2025/01/08/testing-github-integration/">Testing GitHub integration</a></h1>
		

		<a href="https://journal.mackuba.eu/2025/01/08/testing-github-integration/" class="u-url"><time class="dt-published" datetime="2025-01-08 23:27:40 &#43;0200">Jan 8, 2025</time></a>

		<div class="e-content">
			 <p>This is an example post to be saved to GitHub.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://journal.mackuba.eu/2025/01/08/new-domain/">new domain</a></h1>
		

		<a href="https://journal.mackuba.eu/2025/01/08/new-domain/" class="u-url"><time class="dt-published" datetime="2025-01-08 20:49:13 &#43;0200">Jan 8, 2025</time></a>

		<div class="e-content">
			 <p>who dis</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://journal.mackuba.eu/2025/01/08/test-post/">test post</a></h1>
		

		<a href="https://journal.mackuba.eu/2025/01/08/test-post/" class="u-url"><time class="dt-published" datetime="2025-01-08 17:52:09 &#43;0200">Jan 8, 2025</time></a>

		<div class="e-content">
			 <p>test 123</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://journal.mackuba.eu/2025/01/07/hello-world/">Hello world</a></h1>
		

		<a href="https://journal.mackuba.eu/2025/01/07/hello-world/" class="u-url"><time class="dt-published" datetime="2025-01-07 19:55:19 &#43;0200">Jan 7, 2025</time></a>

		<div class="e-content">
			 <p>just setting up my mcroblog</p>

		</div>
	  </div>
	

</div>


    
    

  </body>

</html>
