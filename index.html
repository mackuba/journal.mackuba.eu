<!DOCTYPE html>
<html>

  <head>
	<meta name="generator" content="Hugo 0.91.2" />

  <title>
      
      Kuba&#39;s Journal
      
  </title>

</head>


  <body>

    

	
<div class="h-feed">

	
	
	  <div class="h-entry">
		
			<h1><a href="https://journal.mackuba.eu/2025/06/23/a-couple-of-months-ago/">test speeding up firehose 2</a></h1>
		

		<a href="https://journal.mackuba.eu/2025/06/23/a-couple-of-months-ago/" class="u-url"><time class="dt-published" datetime="2025-06-24 00:53:44 &#43;0200">Jun 24, 2025</time></a>

		<div class="e-content">
			 <p>A couple of months ago, I <a href="/2025/03/18/speeding-up-the-firehose/">wrote about</a> the tests I&rsquo;ve done on how I can speed up my code processing the Bluesky firehose, using my Skyfall Ruby gem. The limits I&rsquo;ve reached then were around:</p>
<ul>
<li>4k events/s with full post processing that I normally do (saving all posts, matching posts to feeds etc.)</li>
<li>5k events/s if using Jetstream - but I noted that it seemed suspiciously like that was a fixed rate limit (which others have confirmed in the comments)</li>
<li>6k events/s just reading packets from the firehose, without processing</li>
</ul>
<p>Since then, I&rsquo;ve done two more things. The first one was that I&rsquo;ve set up a local Jetstream instance for testing, and configured it to have a much higher rate limit (the <a href="https://github.com/bluesky-social/jetstream/blob/d17bd81a945e697b66e682d6ac2364f70e537b27/cmd/jetstream/main.go#L85-L90">&ndash;max-sub-rate option</a>). I&rsquo;ve confirmed that indeed, with Jetstream, Skyfall can go much faster on the same server, up to about <strong>10-12k</strong> doing full processing.</p>
<p>The second thing is that I started doing some profiling to find out where else I could save some processing time, and in the process, I managed to massively speed up the underlying <a href="https://github.com/faye/faye-websocket-ruby">faye-websocket</a> library 🙃</p>
<h2 id="the-faye-speedup-fix">The Faye speedup fix</h2>
<p>So, I was playing with <a href="https://ruby-prof.github.io">ruby-prof</a> to find where else I can shave off a few microseconds. I ran the scan on the version with my processing turned off, expecting the remaining work to be mostly in some boring internals of Faye or Ruby core libs, reading and writing bytes from the socket, adding them together and waiting.</p>
<p>And I found something… quite interesting: a majority of time was spend in two places:</p>
<ol>
<li>some code deep inside Faye&rsquo;s helper library <a href="https://github.com/faye/websocket-driver-ruby/">websocket-driver</a>, which takes a filler string buffer and converts it to a byte array to be dispatched to a handler:</li>
</ol>
<pre tabindex="0"><code>                      4.519      4.519      0.000      0.000    130695/130695     WebSocket::Driver::Hybi#emit_message
  13.83%  13.83%      4.519      4.519      0.000      0.000           130695     String#bytes
</code></pre><ol start="2">
<li>my code in Skyfall which takes a byte array received from Faye and converts it to a binary string:</li>
</ol>
<pre tabindex="0"><code>                      0.000      0.000      0.000      0.000         1/130696     WebSocket::HTTP::Response#body
                     12.549     12.549      0.000      0.000    130695/130696     Skyfall::Firehose#handle_message
  38.42%  38.42%     12.549     12.549      0.000      0.000           130696     Array#pack

</code></pre><p>… wait a moment 🤔🤔💡</p>
<p>Yes, for a binary websocket (which is used here), Faye prepares the received data in a binary <code>String</code>, but then sends it out as an <code>Array</code> of bytes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rb" data-lang="rb"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">emit_message</span>
  message  <span style="color:#f92672">=</span> @extensions<span style="color:#f92672">.</span>process_incoming_message(@message)
  @message <span style="color:#f92672">=</span> <span style="color:#66d9ef">nil</span>

  payload <span style="color:#f92672">=</span> message<span style="color:#f92672">.</span>data

  <span style="color:#66d9ef">case</span> message<span style="color:#f92672">.</span>opcode
    <span style="color:#66d9ef">when</span> <span style="color:#66d9ef">OPCODES</span><span style="color:#f92672">[</span><span style="color:#e6db74">:text</span><span style="color:#f92672">]</span> <span style="color:#66d9ef">then</span>
      payload <span style="color:#f92672">=</span> <span style="color:#66d9ef">Driver</span><span style="color:#f92672">.</span>encode(payload, <span style="color:#66d9ef">Encoding</span><span style="color:#f92672">::</span><span style="color:#66d9ef">UTF_8</span>)
      payload <span style="color:#f92672">=</span> <span style="color:#66d9ef">nil</span> <span style="color:#66d9ef">unless</span> payload<span style="color:#f92672">.</span>valid_encoding?
    <span style="color:#66d9ef">when</span> <span style="color:#66d9ef">OPCODES</span><span style="color:#f92672">[</span><span style="color:#e6db74">:binary</span><span style="color:#f92672">]</span>
      payload <span style="color:#f92672">=</span> payload<span style="color:#f92672">.</span>bytes<span style="color:#f92672">.</span>to_a    <span style="color:#75715e"># &lt;===</span>
  <span style="color:#66d9ef">end</span>
</code></pre></div><p>And since I want a binary string at the end, to pass it to the <a href="https://github.com/cabo/cbor-ruby">CBOR library</a> for decoding, I need to take that byte array and convert it back into a string just like the one we had before:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rb" data-lang="rb"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">handle_message</span>(msg)
  data <span style="color:#f92672">=</span> msg<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>pack(<span style="color:#e6db74">&#39;C*&#39;</span>)
  @handlers<span style="color:#f92672">[</span><span style="color:#e6db74">:raw_message</span><span style="color:#f92672">]&amp;.</span>call(data)
</code></pre></div><p>So could we just… not do that? Yes, although not without some hacking, since the library didn&rsquo;t have an option to emit a string instead for binary websocket messages.</p>
<p>I made some <a href="https://github.com/mackuba/skyfall/blob/0.5.1/lib/skyfall/faye_ext.rb">monkey-patches</a> to Faye &amp; websocket-driver first, and eventually a <a href="https://github.com/faye/websocket-driver-ruby/pull/95">PR</a> which I submitted to the author - adding a <code>:binary_data_format</code> option to <code>Faye::Websocket::Client</code> initializer, where you can ask to have the data returned as a string instead, defaulting to the original method of returning a byte array. The author actually said that he thinks it makes sense to change the default to a binary string (while adding an option to revert it), and version 0.12.0 <a href="https://github.com/faye/faye-websocket-ruby/blob/main/CHANGELOG.md">was released last month</a> with this changed behavior.</p>
<h2 id="new-benchmarks">New benchmarks</h2>
<p>I&rsquo;ve done the benchmarks again, and the results look very encouraging. This is for the CBOR firehose, with and without the fix, and also rechecked the <a href="https://github.com/socketry/async-websocket">async-websocket</a> library for comparison:</p>
<table>
<thead>
<tr>
<th>Library</th>
<th>With processing</th>
<th>With processing, + Rust lib</th>
<th>Just reading</th>
</tr>
</thead>
<tbody>
<tr>
<td>Faye/EM Firehose (old)</td>
<td>3,200-3,350</td>
<td>3,300-3,450</td>
<td>6,800-6,900</td>
</tr>
<tr>
<td>Faye/EM Firehose (+ fix)</td>
<td>5,300-5,600</td>
<td>6,300-6,800</td>
<td>33,000-34,500</td>
</tr>
<tr>
<td>Async Firehose</td>
<td>5,700-5,900</td>
<td>5,700-5,900 (??)</td>
<td>5,800-5,950 (??)</td>
</tr>
</tbody>
</table>
<p>And this is for Jetstream without a rate limit (the Faye fix does <em>not</em> affect/help Jetstream, because here the stream is text-based, not binary, so that problematic code path wasn&rsquo;t used here):</p>
<table>
<thead>
<tr>
<th>Library</th>
<th>With processing</th>
<th>With processing, + Rust lib</th>
<th>Just reading</th>
</tr>
</thead>
<tbody>
<tr>
<td>Faye/EM Jetstream</td>
<td>9,400-9,600</td>
<td>11,000-12,000</td>
<td>180,000-200,000</td>
</tr>
<tr>
<td>Async Jetstream</td>
<td>10,000-11,000</td>
<td>12,000-15,000</td>
<td>250,000-280,000</td>
</tr>
</tbody>
</table>
<p>As you can see:</p>
<ul>
<li>the fix gives me basically 50% speedup for free in the existing live code</li>
<li>with the Rust module for regexp matching turned on, when a higher portion of the whole time is spent inside Faye + Skyfall, that speedup becomes more like 100%</li>
<li>and when you skip all processing, and almost all of the time is spent inside Faye + Skyfall, that gets as much as 4-5x speedup (!)</li>
<li>this gives me a possible ceiling of as much as 33-34k events/s if I manage to further optimize or parallelize the event processing parts (parsing CBOR/CAR, feed matching, building models, calling Postgres etc.)</li>
<li>Jetstream mode, without the rate limit, can be around 2x faster than the Firehose version in practice</li>
<li>with the data saving part optimized further, we can theoretically go into 6 digit numbers with Jetstream, but at this point it&rsquo;s kind of theoretical, because I would likely run into many other bottlenecks before I get there (disk speed, VPS bandwidth limits etc.)</li>
<li>the Async library can be a bit faster than the EventMachine based one, but not dramatically so</li>
</ul>
<p>For unknown reason, I wasn&rsquo;t able to make the Async version go faster than 6k evt/s on the CBOR (binary) stream (while it was using much less than 100% CPU); not sure why, since it worked fine with Jetstream - maybe it was some issue on my side, but I don&rsquo;t really want to spend time digging into this. EM/Faye works fine (especially now), is very battle-tested (even if not being updated much anymore), works with older Rubies, and it would be a big API change for apparently not that much gain. So I think I&rsquo;m going to keep it as is and maybe reconsider for Skyfall 2.0 one day…</p>
<p>Overall, I think all of this gives me more than enough space to not worry about this again until Bluesky becomes <em>much</em> bigger :)
And it looks like I&rsquo;m not even going to need any parallel workers + Redis queue setup anytime soon.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://journal.mackuba.eu/2025/03/18/speeding-up-the-firehose/">Speeding up the firehose</a></h1>
		

		<a href="https://journal.mackuba.eu/2025/03/18/speeding-up-the-firehose/" class="u-url"><time class="dt-published" datetime="2025-03-18 18:26:20 &#43;0200">Mar 18, 2025</time></a>

		<div class="e-content">
			 <p>As the final phase of my <a href="/2025/01/16/postgress-progress/">database experiments</a>, I&rsquo;ve been playing recently with optimizing the speed of processing in the firehose process.</p>
<p>The Bluesky firehose is a streaming API that sends you every single thing happening on the network in real time – every post, like, follow, everything (encoded in a binary format called <a href="https://cbor.io">CBOR</a>). And my firehose process is a client connecting to that stream, pulling data from it and saving the relevant parts (the posts).</p>
<p>At the moment, the firehose flow ranges between ~300 and 700 events per second during the day (&ldquo;event&rdquo; being one &ldquo;action&rdquo; like created or deleted record of any kind), with the bottom around 7-9:00 UTC and the top around 17-22:00 UTC. But the highest bandwidth we&rsquo;ve seen so far was around <a href="https://cdn.bsky.app/img/feed_fullsize/plain/did:plc:oio4hkxaop4ao4wz2pp3f4cr/bafkreickjkjffrmqcq7wdezei2tuytra2hjpnyiesfzc5rkxsobkzpufju@jpeg">1.8-2.0k events/s</a> in mid November, when a massive new wave of new users joined in the aftermath of the US elections, and we&rsquo;ve seen the traffic go up rapidly by a few hundred % within days before too, e.g. in early September when <a href="https://www.cnbc.com/2024/09/04/social-media-platform-bluesky-attracts-millions-in-brazil-after-judge-bans-musks-x-.html">the whole Brazil joined Bluesky</a>. The firehose protocol also includes a lot of extra data, needed for cryptographic verification and full replication of data repositories, but not used in a simple case like this – so the total data bandwidth to stream and process is an average ~20-30 Mbps right now (with the smaller traffic than before).</p>
<p>My server was struggling back then in November, processing the 1.8k events per second, which is a big part of the reason why I started looking for a better suited database than SQLite, to be able to handle the next wave. I had a few improvements lined up, which I now started applying one by one (trying to do one at a time, so I can measure which one does what difference), each time pausing the reading for a day or so, and then catching up for several hours at full speed, so I can check what&rsquo;s the maximum speed it can do.</p>
<p>I was also doing very simple profiling on the way, measuring which parts take how much time and where I can still save some extra microseconds:</p>
<!-- raw HTML omitted -->
<p>Overall, I managed to bring the number from around 2k evt/s to around 4k over a period of about a week (similarly in the MySQL and Postgres versions) – so not bad at all.</p>
<!-- raw HTML omitted -->
<p>These are the changes I&rsquo;ve made:</p>
<hr>
<h3 id="partial-indexes">Partial indexes</h3>
<p>I&rsquo;ve switched two indexes in Postgres to <a href="https://www.postgresql.org/docs/current/indexes-partial.html">partial indexes</a> (MySQL doesn&rsquo;t support that). I have two indexes in the posts table, on <code>quote_id</code> and <code>thread_id</code>, which set a reference to a quoted post and the root of the thread respectively, which are NULL for some of the posts (<code>thread_id</code> for like half of the posts, <code>quote_id</code> for the vast majority), so I added a <code>WHERE ... IS NOT NULL</code> condition to those two.</p>
<p><strong>Effect:</strong> no noticeable change in performance, but it least the indexes take less space now.</p>
<hr>
<h3 id="activerecord">ActiveRecord</h3>
<p>I updated ActiveRecord – from v. 6.1 that I had before to 7.2 (Rails 8 was released in November, but it feels a bit too fresh right now). Ironically, AR 8.0 seems to finally <a href="https://github.com/rails/rails/pull/50371">fix my longstanding problem</a> with concurrency errors in SQLite, just when I&rsquo;m getting rid of it…</p>
<p><strong>Effect:</strong> around +10% processing speed.</p>
<hr>
<h3 id="ruby-update--yjit">Ruby update + YJIT</h3>
<p>I updated Ruby from 3.2 to 3.4 and enabled <a href="https://shopify.engineering/ruby-yjit-is-production-ready">YJIT</a>, a new optional JIT written in Rust. I had tested YJIT before on 3.2 and there was a noticeable improvement, but I&rsquo;ve noticed some kind of memory leak then that they&rsquo;ve apparently fixed only around <a href="https://bugs.ruby-lang.org/issues/20209">3.3.1</a>. Interestingly, my code seems to run slightly <em>slower</em> on 3.3 and 3.4 without YJIT, but enabling it more than makes up for it.</p>
<p><strong>Effect:</strong> around +15%.</p>
<hr>
<h3 id="batch-inserts">Batch inserts</h3>
<p>I was already previously grouping posts into a queue, waiting until I got 10, and then saving them all in one transaction, using code like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rb" data-lang="rb"><span style="color:#66d9ef">ActiveRecord</span><span style="color:#f92672">::</span><span style="color:#66d9ef">Base</span><span style="color:#f92672">.</span>transaction <span style="color:#66d9ef">do</span>
  @post_queue<span style="color:#f92672">.</span>each <span style="color:#66d9ef">do</span> <span style="color:#f92672">|</span>p<span style="color:#f92672">|</span>
    p<span style="color:#f92672">.</span>save!
  <span style="color:#66d9ef">end</span>
<span style="color:#66d9ef">end</span>
</code></pre></div><p>This was already faster than the initial way of saving of each post separately, directly in the block where it&rsquo;s built from the processed event. But that was still multiple INSERTs, and I wasn&rsquo;t sure I could do it in just one while using the ORM.</p>
<p>Apparently I can, with the use of the <a href="https://dev.to/lorankloeze/creating-lots-of-records-in-rails-with-insertall-1bhe">insert_all API</a>. The catch is that it takes an array of raw argument hashes, skips validations and doesn&rsquo;t handle child records (my Post records have linked FeedPost entries if they are included in a feed). So I validate the records separately earlier (I was actually already validating them first before, which means the validations were running twice), and for those that have FeedPosts, I save them separately next.</p>
<p>So the whole thing looks like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rb" data-lang="rb"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">process_post</span>(msg)
  post <span style="color:#f92672">=</span> <span style="color:#66d9ef">Post</span><span style="color:#f92672">.</span>new(<span style="color:#f92672">...</span>)

  <span style="color:#66d9ef">if</span> <span style="color:#f92672">!</span>post<span style="color:#f92672">.</span>valid?
    log <span style="color:#e6db74">&#34;Error: post is invalid: ...&#34;</span>
    <span style="color:#66d9ef">return</span>
  <span style="color:#66d9ef">end</span>

  @post_queue <span style="color:#f92672">&lt;&lt;</span> post

  <span style="color:#66d9ef">if</span> @post_queue<span style="color:#f92672">.</span>length <span style="color:#f92672">&gt;=</span> <span style="color:#66d9ef">POSTS_BATCH_SIZE</span>
    save_queued_posts
  <span style="color:#66d9ef">end</span>
<span style="color:#66d9ef">end</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">save_queued_posts</span>
  matched, unmatched <span style="color:#f92672">=</span> @post_queue<span style="color:#f92672">.</span>partition { <span style="color:#f92672">|</span>x<span style="color:#f92672">|</span> <span style="color:#f92672">!</span>x<span style="color:#f92672">.</span>feed_posts<span style="color:#f92672">.</span>empty? }

  <span style="color:#66d9ef">if</span> unmatched<span style="color:#f92672">.</span>length <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>
    values <span style="color:#f92672">=</span> unmatched<span style="color:#f92672">.</span>map { <span style="color:#f92672">|</span>p<span style="color:#f92672">|</span> p<span style="color:#f92672">.</span>attributes<span style="color:#f92672">.</span>except(<span style="color:#e6db74">&#39;id&#39;</span>) }
    <span style="color:#66d9ef">Post</span><span style="color:#f92672">.</span>insert_all(values)
  <span style="color:#66d9ef">end</span>

  @post_queue <span style="color:#f92672">=</span> matched
  <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">if</span> @post_queue<span style="color:#f92672">.</span>empty?

  <span style="color:#66d9ef">ActiveRecord</span><span style="color:#f92672">::</span><span style="color:#66d9ef">Base</span><span style="color:#f92672">.</span>transaction <span style="color:#66d9ef">do</span>
    @post_queue<span style="color:#f92672">.</span>each <span style="color:#66d9ef">do</span> <span style="color:#f92672">|</span>p<span style="color:#f92672">|</span>
      p<span style="color:#f92672">.</span>save!(<span style="color:#e6db74">validate</span>: <span style="color:#66d9ef">false</span>)
    <span style="color:#66d9ef">end</span>
  <span style="color:#66d9ef">end</span>

  @post_queue <span style="color:#f92672">=</span> <span style="color:#f92672">[]</span>
<span style="color:#66d9ef">rescue</span> <span style="color:#66d9ef">StandardError</span> <span style="color:#f92672">=&gt;</span> e
  <span style="color:#75715e"># ... try to save valid posts one by one</span>
<span style="color:#66d9ef">end</span>
</code></pre></div><p><strong>Effect:</strong> up to around +30% in the Postgres version, less in MySQL one.</p>
<hr>
<h3 id="rust-native-module-for-regex-matching">Rust native module for regex matching</h3>
<p>Last year I wrote a little native module in Rust (which I didn&rsquo;t know at all before) to speed up matching of post contents with a large number of regexps for the feeds:</p>
<!-- raw HTML omitted -->
<p>The results were… unexpected. Swift &amp; Crystal didn&rsquo;t do well, but JS &amp; PHP did 🤔<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->[image or embed]<!-- raw HTML omitted --><!-- raw HTML omitted -->— Kuba Suder 🇵🇱🇺🇦 (<!-- raw HTML omitted --><a href="http://mackuba.eu">@mackuba.eu</a><!-- raw HTML omitted -->) <!-- raw HTML omitted -->April 16, 2024 at 7:34 PM<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>I haven&rsquo;t enabled it on production yet though, because there are some edge cases I need to get fixed, and I only used it for manual feed rebuilding so far. So as the next step, I enabled that Rust module in the firehose process. (I&rsquo;d love to eventually make that available as a gem you can easily add to your app.)</p>
<p><strong>Effect:</strong> +15%.</p>
<hr>
<h3 id="asynchronous-transaction-commits">Asynchronous transaction commits</h3>
<p>Next, I turned on two options: <code>synchronous_commit = off</code> in Postgres, and <code>innodb-flush-log-at-trx-commit = 0</code> in MySQL. These basically make it so that when you commit a transaction, you don&rsquo;t wait until it&rsquo;s synced to disk first, but the request returns just leaving the data in memory, and the data is synced to disk later (but like, up to a second later). So if the whole instance hard-reboots for some reason, you can lose maybe one second of data (but it wouldn&rsquo;t corrupt the database, since the transaction is either saved or not).</p>
<p>This is possibly still dangerous if you&rsquo;re saving user&rsquo;s private data from a webapp, but in this case, I&rsquo;m streaming public data that I can scroll back through a bit if needed. So in the highly unlikely case that I lose a second of data somehow, I just replay a very slightly older cursor.</p>
<p><strong>Effect:</strong> no noticeable improvement… probably because with the batched inserts, it was already doing just a few inserts per second, so there wasn&rsquo;t any more time to save here.</p>
<hr>
<h3 id="optimizing-json-handling-and-time-parsing">Optimizing JSON handling and Time parsing</h3>
<p>I had a place in the processing function where it encodes a record parsed from CBOR into JSON, to be saved into a <code>data</code> column and for logging purposes, but for complicated reasons that <code>JSON.generate</code> was called twice with slightly different versions of record. So I refactored that to let it skip one encode.</p>
<p>There was also some time spent parsing the record time and the event time into <code>Time</code> objects from an ISO8601 string using <code>Time.parse</code>; but I had a feeling that this could probably be speeded up using some native code, especially if it <em>only</em> accepts ISO8601 formats, and only in forms that actually appear in the data (since <code>Time.parse</code> handles many, many possible formats). With the help of Mr. GPT I made a little C module with a single function to replace <code>Time.parse</code>, and that indeed ended up being faster. (I&rsquo;ll also try to turn that into a micro-gem.)</p>
<p><strong>Effect:</strong> +8% (this finally pushed it over the 4k level)</p>
<hr>
<h3 id="jetstream">Jetstream</h3>
<p>Finally, just to check for now, I&rsquo;ve switched from the full CBOR firehose to <a href="https://github.com/bluesky-social/jetstream">Jetstream</a>, which is a kind of pre-processed stream that strips all the signatures and Merkle tree stuff and outputs a simplified JSON stream. I&rsquo;ve already added support for it to my firehose library <a href="https://github.com/mackuba/skyfall">Skyfall</a> in October, but I&rsquo;m not using it everywhere yet.</p>
<p><strong>Effect:</strong> +30-40%, although the speed was suspiciously capped at almost precisely ~4989 evt/s on both servers… so I&rsquo;m guessing I might have hit some kind of rate limit of the Jetstream server. But I could also host it myself if needed.</p>
<p>(Edit: yes, apparently 5k is the <a href="https://bsky.app/profile/edavis.dev/post/3lkobu26k222n">default rate limit</a> configured in Jetstream server settings.)</p>
<hr>
<h3 id="potential-ceiling">Potential ceiling</h3>
<p>I also did a test with all processing completely skipped, so the app would just read the data packets from the websocket and discard them, not even decode the CBOR. This did around 6k evt/s with the full firehose (possibly more for Jetstream, but I couldn&rsquo;t test this) – so it looks like this is the max I can do without switching to something completely different. The <a href="https://github.com/faye/faye-websocket-ruby">library I&rsquo;m using</a> for websocket connection uses the very old <a href="https://github.com/eventmachine/eventmachine">eventmachine</a> library underneath, and I was told that the newer <a href="https://github.com/socketry/async-websocket">async-websocket</a> that uses the <a href="https://github.com/socketry/async">Async framework</a> could be faster, but in brief tests I wasn&rsquo;t able to see any improvement so far.</p>
<p>Finally, just to see what the limit would be if I were to abandon Ruby (at least for that part), I ran a quick test in Rust, specifically using the example from the <a href="https://github.com/sugyan/atrium/tree/main/examples/firehose">Atrium library</a>. It did… 25k evt/s 😱 So yeah, there are still more options ;)</p>
<p>That said, my <a href="https://www.netcup.com/en/server/root-server">current VPS</a> will max out the allowed traffic at a sustained 300 Mbps, which if I&rsquo;m counting correctly would be something like 7.8k evt/s from the full firehose, so above that level this would be the main problem.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://journal.mackuba.eu/2025/03/06/macbook-air-vs-pro/">MacBook Air vs. Pro</a></h1>
		

		<a href="https://journal.mackuba.eu/2025/03/06/macbook-air-vs-pro/" class="u-url"><time class="dt-published" datetime="2025-03-06 19:05:06 &#43;0200">Mar 6, 2025</time></a>

		<div class="e-content">
			 <p>I&rsquo;m a very picky person when it comes to buying things… especially Apple hardware. I can&rsquo;t count the number of Apple devices I tried out and sent back, because something wasn&rsquo;t right. Because there are always tradeoffs, and I don&rsquo;t like tradeoffs, I want it to be just perfect 😅</p>
<p>So since I really needed a new MacBook (I&rsquo;ve been using an M1, which would be fine if I had only bought it with more than 8 GB RAM… 🤦🏻‍♂️), last autumn I decided to test MacBooks this way. I wasn&rsquo;t sure if I wanted an Air or a Pro, since they&rsquo;re quite different right now. There&rsquo;s this <a href="https://www.macrumors.com/2024/11/08/apples-extended-return-policy-2024/">holiday promotion</a> that Apple has each year around Black Friday, that if you buy something in November-December, you can safely return it until the 2nd week of January (!) instead of the usual 2-week period. So I decided to buy both an M3 13&quot; Air and an M4 14&quot; Pro, try them out and pick one to keep in January 🙃</p>
<p>Spoiler: I&rsquo;ve kept the Air in the end. The Pro has some things better than the Air, but it&rsquo;s noticeably thicker and heavier, and using the classic MacBook Airs for ~5 years in the last decade has spoiled me forever, and I just refuse to go back to heavier, bulkier laptops.</p>
<p>There are a few more differences that I wrote down, so here&rsquo;s a short subjective comparison if you&rsquo;re just looking at those shiny new <a href="https://www.macrumors.com/2025/03/05/apple-announces-new-macbook-air-with-m4-and-sky-blue-color-option/">Sky Blue Airs</a> and thinking if you should get that one or the Pro…</p>
<p>Note: I haven&rsquo;t really used the Pro that much, certainly not all the time for 2 months – more like a few times for a few hours. I&rsquo;m writing on the Air right now.</p>
<h3 id="size--exterior">Size / exterior</h3>
<p>As mentioned, there is definitely a difference. For comparison:</p>
<ul>
<li>current MacBook Air 13.6&quot; (2025): 1.24 kg</li>
<li>Retina MacBook Air 13.3&quot; (2020): 1.29 kg</li>
<li>classic MacBook Air 13.3&quot; (2015): 1.35 kg</li>
<li>older MacBook Pro 13.3&quot; (2020): 1.40 kg</li>
<li>current MacBook Pro 14.2&quot; (2024): 1.55 kg (M4) to 1.62 kg (M4 Max)</li>
</ul>
<p>So the lightest Pro is 150g heavier than my previous M1, 200g heavier than classic MacBook Airs, and 300g+ heavier than the current Air (!). That&rsquo;s about 2/3 of the iPad&rsquo;s weight of difference, and it&rsquo;s absolutely noticeable. It might not be a problem if you use it as a desktop computer, but it is a problem if you need to carry it around all the time, or if you have the &ldquo;on the lap lying on the couch&rdquo; kind of workplace like me…</p>
<p>The Pro is also a lot thicker (and at least feels thicker than the 13.3&quot; Pro). It&rsquo;s not that much of a problem, but I definitely like how super thin the Air is.</p>
<p>I remember the Pro also had those kinda annoying slots with a bit sharp edges at the bottom, for air in/outflow probably, Air doesn&rsquo;t have those.</p>
<h3 id="screen">Screen</h3>
<p>The Pro has a slightly bigger screen, not that much though.</p>
<p><strong>ProMotion</strong> (120 Hz) on the Pro is immediately noticeable, but I can&rsquo;t say if it&rsquo;s important or even clearly better, it&rsquo;s a bit weird at first (feels kinda… iOS-y?); probably something I could get used to rather quickly though. I can&rsquo;t say I&rsquo;m missing it on the Air at all, but I&rsquo;m not <em>that</em> used to this mode, since the only devices with it that I owned was my previous and current iPad.</p>
<p>The Pro has a better <strong>DPI</strong> – the screen has a &ldquo;full 2x Retina&rdquo; resolution (1512 x 982 &ldquo;point&rdquo; size), while the Air uses a scaled resolution (1280 x 832 &ldquo;point&rdquo; size, while the standard display resolution is 1470 x 956), so the Pro&rsquo;s screen is sharper. Again, the past two MacBooks I&rsquo;ve used also used the scaled resolution, so it&rsquo;s not something that&rsquo;s bothering me.</p>
<p>Pro has an <strong>HDR mode</strong> with higher brightness – I&rsquo;ve tested this playing &ldquo;Silo&rdquo; on Apple TV, but… I had a bit mixed feelings about it. I&rsquo;m not sure if it looked better on the Pro with the higher contrast and real black, than on the Air with less black blacks. And the white interface elements that show up on hover were kinda blinding.</p>
<p>The Pro is technically capable of higher peak <strong>SDR brightness</strong>, but it&rsquo;s hard to say how this works exactly – the specs say &ldquo;up to 1000 nits (outdoor)&rdquo; (vs. Air&rsquo;s 500 nits). But note the &ldquo;outdoor&rdquo; – so it&rsquo;s something that uses the light sensor, and only enables higher max limit if it&rsquo;s in a bright place. So if you took both of them outside on a bright day, the Pro would probably be much more usable, but in a rather dark room, they looked pretty much the same at max brightness. But there might be some hacks to unlock the limit (?). Anyway, as someone who usually has max brightness set, and has sent back one Air in the past because it was too dark (400 nits), the current Air is good enough for me.</p>
<p>I was a bit worried about the screen <strong>color balance</strong> – Apple has this annoying thing where every device has a <em>slightly</em> different white tone, sometimes it&rsquo;s more yellowish, sometimes more blueish, sometimes more towards violet…. and it&rsquo;s not always possible/easy to calibrate it differently. Some of the past MacBook Airs and the new OLED iPads are too yellowish for me, for example.</p>
<p>So these two… are both kinda yellowish, the Pro even more so I think. But they can be calibrated a bit – the Pro has this <a href="https://journal.mackuba.eu/uploads/2025/screenshot-2024-12-16-at-19.27.11.png">more advanced color calibration thingie</a> that I&rsquo;ve never seen before, that&rsquo;s missing on the Air, which lets you change color balance on a hexagonal grid. The Air has a standard color profile wizard with a white point slider. But I&rsquo;ve moved it <em>slightly</em> towards the blue and it&rsquo;s ok. If you&rsquo;re used to Apple&rsquo;s current OLED screens (I&rsquo;m not), you won&rsquo;t notice this.</p>
<h3 id="notch">Notch</h3>
<p>I was horrified when they first announced the MacBook with a notch in the screen… I was expecting this to be a big problem. It&rsquo;s not really a problem in practice, and I say this as a notch-hater. I know there&rsquo;s <a href="https://apps.apple.com/pl/app/say-no-to-notch/id1639306886?mt=12">some hack</a> that lets you run a smaller resolution, pushing the menu bar down and making the screen fully rectangular, at the cost of lost vertical space, but I don&rsquo;t think I&rsquo;d want that – those pixels definitely count. The only thing I don&rsquo;t like is that the notch <a href="https://bsky.app/profile/mackuba.eu/post/3lhk7xzg3k22x">swallows overflowing icons</a> from the menu bar, they just disappear if you have too many. I was recommended several tools that work around this in that thread, but I haven&rsquo;t tried any yet.</p>
<h3 id="keyboard">Keyboard</h3>
<p>I think both have fairly similar keyboards – the good one that Apple switched back to from the terrible butterfly one. They both felt ok to write on.</p>
<p>However, they do <em>look</em> different – the Air has the classic black keys on silver background design, while the Pro has black keys on… black. I&rsquo;m not a fan of this, tbh – the keys are much harder to see. They do have the backlight of course, but you kinda need to rely only on that.</p>
<h3 id="ports">Ports</h3>
<p>The Pro has an HDMI port and an SD card slot. I couldn&rsquo;t care less tbh – I use these very rarely, and I got adapters. I prefer a thinner laptop instead.</p>
<p>The bigger difference is the extra USB-C port on the right, while the Air only has ports on the left – that would be kinda useful, but oh well.</p>
<p>The Pro also has the headphone jack on the left instead of right, but I&rsquo;m not sure if that matters. It might be better this way tbh – I think they used to be on the left long ago, and I liked that more?…</p>
<h3 id="temperature">Temperature</h3>
<p>The Air definitely runs a bit warmer at idle than the M1 MBP did (doesn&rsquo;t really fall below 40°C when it&rsquo;s running), and gets warm quicker when doing something serious, and I don&rsquo;t really like this… or even if the temperature of the CPU is the same, it <em>feels</em> warmer on the bottom, which matters when you&rsquo;re mostly holding it on the lap. I guess it might be because of the fan-less internal design, or just things are arranged differently inside somehow. Although I think the Pro was also slightly warmer than my M1. (I&rsquo;m curious how the new M4 Air will turn out in complete reviews…)</p>
<h3 id="sound">Sound</h3>
<p>The Pro had a better sound on the speakers – not that the Air sounds bad, but the Pro definitely sounded better somehow (note, I&rsquo;m not an audiophile at all).</p>
<p>And surprisingly, the Air has a different sounding touchpad. I think the click sound is the same here as on the 2020 MacBook Air, and different than the current and last few generations of Pros – the Pros have a kinda more &ldquo;metallic&rdquo; click sound, and the Airs' touchpad click is… I don&rsquo;t know how to describe it, it&rsquo;s different. A slightly lower, more &ldquo;dull&rdquo; tone. I think I like the Air version more.</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://journal.mackuba.eu/2025/02/10/year-review/">Year 2024 review</a></h1>
		

		<a href="https://journal.mackuba.eu/2025/02/10/year-review/" class="u-url"><time class="dt-published" datetime="2025-02-10 19:42:44 &#43;0200">Feb 10, 2025</time></a>

		<div class="e-content">
			 <p>2024 was a pretty intense year for me in terms of coding… and a pretty not-intense one for anything else 🫣 I was so completely absorbed with building my Bluesky-related projects. It gives me a lot of satisfaction to be able to work on things which are based on my own ideas and needs, which give me fun problems to solve, let me learn a lot, and which seem to be helping <a href="https://bsky.app/profile/mackuba.eu/post/3lh5vqe5yjs2r">a lot of other people</a>.</p>
<p>I figured I could look back at this past year and make a summary of what I managed to build and do, since it&rsquo;s been a while since I&rsquo;ve done <a href="https://mackuba.eu/2024/03/27/march-projects-update/">one of those</a>. Yes, I know we&rsquo;re well into February already, I&rsquo;ve been procrastinating, sue me. (Also, we&rsquo;ve just had an anniversary of <a href="https://bsky.social/about/blog/02-06-2024-join-bluesky">Bluesky public launch</a> a few days ago, so that seems like a good excuse too.)</p>
<hr>
<h3 id="bluefeeds">Bluefeeds</h3>
<p>Bluefeeds is my internal codename for the Ruby project which runs my main server at <a href="https://blue.mackuba.eu">blue.mackuba.eu</a>, which includes my feed service, the Handle Directory, statistics and so on.</p>
<ul>
<li>New feeds: <a href="https://bsky.app/profile/mackuba.eu/feed/atproto">ATProto feed</a>, <a href="https://bsky.app/profile/mackuba.eu/feed/atproto-links">ATProto projects feed</a>, <a href="https://bsky.app/profile/mackuba.eu/feed/kit">Kit feed 🐱</a>, <a href="https://bsky.app/profile/mackuba.eu/feed/newsmastodon">News From Mastodon</a>, <a href="https://bsky.app/profile/mackuba.eu/feed/teamlinks">Bluesky Team Links feed</a></li>
<li>The two Replies feeds: <a href="https://bsky.app/profile/mackuba.eu/feed/replies">Only Replies</a> and <a href="https://bsky.app/profile/mackuba.eu/feed/follows-replies">Follows &amp; Replies</a>, which have gained a lot of popularity after the <a href="https://bsky.app/profile/bsky.app/post/3l2s5t6op2w2x">Following feed changes</a> in the summer, when the option to show all replies in Following was removed, and which are now my two feeds with the biggest traffic</li>
<li>A work-in-progress <a href="https://bsky.app/profile/mackuba.eu/feed/hashtag">Hashtags feed</a> (for following hashtags, currently requires <a href="https://bsky.app/profile/mackuba.eu/post/3lhahkr4n4c2y">running a Ruby script</a> to add tags)</li>
<li>Some continuous updates to existing feeds, like the <a href="https://bsky.app/profile/mackuba.eu/feed/linux">Linux feed</a>, where I&rsquo;ve recently had to fix the name &ldquo;RedHat&rdquo; appearing in <a href="https://bsky.app/profile/mackuba.eu/post/3lhcrmbsbuc2k">completely unrelated contexts</a> 🫠</li>
<li>The <a href="https://blue.mackuba.eu/directory/">Handle Directory</a>, listing handles grouped by top-level domain, a global list of most popular accounts with custom handles, and a ranking of independent PDSes</li>
<li>The <a href="https://blue.mackuba.eu/stats/">statistics page</a>, showing daily and weekly post stats, later also separately for bridged posts from Mastodon/Nostr and posts on independent PDSes
<ul>
<li>I also periodically post stats of <a href="https://bsky.app/profile/mackuba.eu/post/3lge5635azs2g">most popular languages on Bluesky</a> – what % of posts are in which language, and how many users are there that regularly post in each</li>
</ul>
</li>
<li>The very unstyled <a href="https://blue.mackuba.eu/labellers/">labellers list page</a>, which was even briefly featured in a <a href="https://www.youtube.com/watch?v=Kbk1QyS0B_0&amp;t=625s">CNBC video</a> for a few seconds 🤯</li>
<li>An automatically updating list of official <a href="https://bsky.app/profile/mackuba.eu/lists/3lcq5ovmsjn2s">.gov</a> and <a href="https://bsky.app/profile/mackuba.eu/lists/3lcq7vydkgi27">.edu</a> accounts with verified handles, plus an open source repo with the <a href="https://github.com/mackuba/userlist_sync">script that updates them</a></li>
<li>I also managed to learn some basics of Rust 🦀 and to write – with major help from ChatGPT – a tiny native module for Ruby in Rust, which I knew absolutely nothing about before, to speed up the matching of post contents to regexps (I&rsquo;ll try to write a bit more about this at some point and share the code)</li>
<li>… and I&rsquo;ve spent literally months working on migrating this whole thing from SQLite to <a href="https://journal.mackuba.eu/2025/01/16/postgress-progress/">either MySQL or Postgres</a>, and I&rsquo;m still not done 😩</li>
</ul>
<h3 id="skythreadhttpsgithubcommackubaskythread"><a href="https://github.com/mackuba/skythread">Skythread</a></h3>
<ul>
<li>Added a hashtag feed view, listing latest posts with the given hashtag, back when this wasn&rsquo;t working in the Bluesky app yet</li>
<li>Similarly, added a post quotes counter and a view listing the post quotes, before that was added to the app (you can still use it now to peek at the quotes which are hidden/detached in the official app 😈)</li>
<li>Implemented link highlighting and link cards</li>
<li>Added special handling for bridged Mastodon posts, which are showing author&rsquo;s formatted Mastodon handle and the expanded full-length post contents (since Bridgy <a href="https://github.com/snarfed/bridgy-fed/issues/1092#issuecomment-2164027121">stashes the full original text</a> in a custom field in the post records)</li>
<li>Added an &ldquo;<a href="https://bsky.app/profile/mackuba.eu/post/3kxg32sjqi22x">infohazard</a>&rdquo; feature, which lets you load hidden comments which the official app hides because of the &ldquo;nuclear block&rdquo; between two commenters</li>
</ul>
<p>Some of these, like the quotes list and the hidden comments, use data loaded from the Bluefeeds server APIs – since I&rsquo;m saving all incoming posts to one big-ass table, I can index them by quote source and by thread root, and can use that to look up all quotes of a post or all replies which are supposed to be there in the thread.</p>
<h3 id="blog">Blog</h3>
<ul>
<li>I wrote a super long post &ldquo;<a href="https://mackuba.eu/2024/02/21/bluesky-guide/">Complete guide to Bluesky</a>&rdquo; with a lot of explanations of how things work and various tips and tricks (and updated it a few times, as new features were added)</li>
</ul>
<h3 id="skyfallhttpsgithubcommackubaskyfall"><a href="https://github.com/mackuba/skyfall">Skyfall</a></h3>
<ul>
<li>7 releases, adding support for new features like labellers or account state events</li>
<li>added support for streaming from <a href="https://github.com/bluesky-social/jetstream">Jetstream</a>, parsing events from JSON instead of CBOR</li>
</ul>
<h3 id="miniskyhttpsgithubcommackubaminisky"><a href="https://github.com/mackuba/minisky">Minisky</a></h3>
<ul>
<li>2 releases with small improvements (OAuth support is still waiting for a better day)</li>
</ul>
<h3 id="new-projects--tools">New projects &amp; tools</h3>
<ul>
<li><a href="https://blue.mackuba.eu/scanner/">Label Scanner</a> – a simple web tool that shows all labels assigned to an account/post by any of the known labellers</li>
<li><a href="https://sdk.blue">sdk.blue</a> – list of libraries and SDKs for Bluesky/ATProto, grouped by language</li>
<li><a href="https://github.com/mackuba/tootify">tootify</a> – a simple tool for selective cross-posting from Bluesky to Mastodon, which I use to mirror some of my Bluesky posts to my <a href="https://martianbase.net/@mackuba">Mastodon account</a></li>
<li><a href="https://github.com/mackuba/didkit">didkit</a> – a Ruby library for resolving handles to DIDs, loading DID info like assigned PDS host etc.</li>
<li>a <a href="https://gist.github.com/mackuba/e7e865dfbc0d978a2ac21d02566b2e8f">bookmarklet</a> to easily mute notifications in the Bluesky web app</li>
<li>two Ruby scripts to scan <a href="https://gist.github.com/mackuba/711764ea0bfd058610cc203a6a3f834f">your home feed</a> or <a href="https://gist.github.com/mackuba/99110c07736570c02220038c1b716817">some selected accounts</a> and print the stats of who is posting how much daily</li>
</ul>
<h3 id="servers">Servers</h3>
<ul>
<li>I&rsquo;ve migrated my projects from my old Swiss hosting <a href="https://coin.host">Coin.host</a> to German <a href="https://www.netcup.com/en">Netcup</a>, which has VPSes with <em>really</em> fast CPUs and SSDs and a ton of bandwidth included for a really good price</li>
<li>I&rsquo;ve set up my own self-hosted PDS at <strong>lab.martianbase.net</strong>, and <a href="https://internect.info/did/did:plc:oio4hkxaop4ao4wz2pp3f4cr">migrated</a> my main account there</li>
</ul>
<hr>
<p>Oof… that&rsquo;s a lot of stuff 🥲 What&rsquo;s next, in 2025? I… I really think I need to work a bit more on work-life balance (sideproject-life balance?) this year. I feel like I&rsquo;m doing way too many things. But I have no idea if that&rsquo;s going to happen. I think at least I should try to do a bit more project finishing and a bit less project starting.</p>
<p>First of all, I can&rsquo;t wait to get out of this transitional state with that main service running on three different databases in parallel. I think I&rsquo;m really close now (though I&rsquo;ve been saying this for weeks).</p>
<p>Then, I have all of the above to maintain, OAuth to add to Minisky &amp; Skythread, things like better documentation and tests to add to the open source projects. I have a few ideas for longer blog posts to write, e.g. some kind of architectural introduction to ATProto.</p>
<p>And also… I&rsquo;d really love to write an <a href="https://atproto.com/guides/glossary#app-view">AppView</a> one day (but yes, I know, less project starting 😛).</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://journal.mackuba.eu/2025/01/16/postgress-progress/">Postgress progress</a></h1>
		

		<a href="https://journal.mackuba.eu/2025/01/16/postgress-progress/" class="u-url"><time class="dt-published" datetime="2025-01-16 22:39:26 &#43;0200">Jan 16, 2025</time></a>

		<div class="e-content">
			 <p>I think I&rsquo;ve finally made some good progress with my Postgres database. Things are working pretty well now, and I still have a few fixes in the queue. It&rsquo;s not all finished yet, but I feel like I&rsquo;m coming out of the woods now.</p>
<p>For those out of the loop: I have a server which streams and saves all posts from the Bluesky firehose, and uses this data to run several feeds on Bluesky used by 1k+ people, plus generates daily post statistics, the directory of custom handles and things like that.</p>
<p>This all has been running on SQLite so far (200+ GB file now), and it has served me well far longer than I would ever have expected, but in the end I&rsquo;m hitting its limits. The main problem is lack of support for concurrent writes, which makes it unrealistic to e.g. split the work into multiple workers, which I&rsquo;m gonna have to do sooner or later.</p>
<p>I&rsquo;ve only really used MySQL before on servers, but everyone was telling me I need to use Postgres, so I did what any normal person would have done (right?): I&rsquo;ve set up two test servers, one on MySQL and one on Postgres, running from two modified branches of the code, and I made the SQLite-based production server proxy some requests to one or the other in order to test them on real traffic. A sort of database A/B testing 🤪</p>
<p>It has been working like this for a few months now, and I&rsquo;ve been slowly tweaking things, mostly on the Postgres side. Since I got it to work at all, they&rsquo;ve both been working ok, but MySQL has generally been doing more write I/O doing the same things, and Postgres has been doing comparably more read I/O.</p>
<p>The key part are the Replies feeds (<a href="https://bsky.app/profile/did:plc:oio4hkxaop4ao4wz2pp3f4cr/feed/follows-replies">Follows &amp; Replies</a> and <a href="https://bsky.app/profile/did:plc:oio4hkxaop4ao4wz2pp3f4cr/feed/replies">Only Replies</a>, it&rsquo;s basically the same code). These two are kind of variants of the built-in Following feed, showing the connecting user their own chronological timeline, generated live, filtered in some way. These feeds work by fetching the requesting user&rsquo;s follows list, either from the AppView or from a local cache, and then making a query that&rsquo;s basically: <em>&ldquo;give me the most recent 100 posts from any of these 50/200/1000/5000 users&rdquo;</em>.</p>
<p>There are two relevant indexes in that table: one on <code>(repo, time)</code> (repo = user&rsquo;s DID), and one on just <code>(time)</code>. Roughly speaking, for those users who follow e.g. 80 or 200 people, it makes more sense to scan the <code>(repo, time)</code> index those 80-200 times and collect the 100 most recent posts from all of those found, and for those who follow e.g. 9000 (yes, that happens 😛), it&rsquo;s faster to scan the single <code>(time)</code> index until you find 100 relevant posts. But I&rsquo;ve been struggling to make Postgres always use the right index.</p>
<p>At first it tried to use the <code>(time)</code> index way too much. I tried to find a way to force it to use a specific index, but I was told it&rsquo;s not possible (<a href="https://pg-hint-plan.readthedocs.io">it might be though!</a>). With the help of some people on Bluesky, I managed to rewrite the query to use the other index, but then it worked bad for those people with a lot of follows. I interpreted the EXPLAINs as the query planner not having a good enough info about the structure of the data (it estimated <code>n_distinct</code>, the total number of distinct values in a column, as much lower than the actual number of users in the table), so I started messing with the &ldquo;statistics&rdquo; settings. I ended up bumping up the statistics target to the maximum 10,000 (making ANALYZE take really long) and hardcoded <code>n_distinct</code> to 4.5M, which was closer to the total count. I also increased <code>shared_buffers</code> to 1 GB. This improved things somewhat, to the &ldquo;not great, not terrible&rdquo; level, but I was still not happy with the performance.</p>
<p>But eventually, I was able to improve things a lot over the last few weeks:</p>
<ul>
<li>
<p>First, I was re-reading one of the blog posts about the differences in how MySQL and Postgres organize data internally, and I had a bit of an epiphany: unlike MySQL, Postgres doesn&rsquo;t store the primary key (<code>id</code>) in secondary indexes, since they reference the physical location of the data instead. In MySQL I had a version of the query which first did a subquery that collected just post IDs, and then used the final 100 IDs to fetch posts, but this couldn&rsquo;t work here if the indexes don&rsquo;t include <code>id</code>! This was something unexpected for me. So I replaced the larger index with <code>(repo, time DESC, id)</code> and rewrote the query again with a subquery, and the EXPLAIN showed &ldquo;Index Only Scan&rdquo; now, meaning it could use just the index for the subquery.</p>
</li>
<li>
<p>Next, the Index Only Scan was showing something like: &ldquo;Heap Fetches: 31362&rdquo;. This apparently means that even though it tries to only use the index in that step, it still has to go fetch 30k post rows from the actual table. This is because as records are updated or deleted (in my case only really deleted), Postgres just marks them as deleted, but doesn&rsquo;t free up the space or update the index until a VACUUM is done. A separate structure called &ldquo;visibility map&rdquo; tracks which table pages have some outdated records in them, and if an index entry points to such page, it has to go check if that record is still there. So I started <a href="https://bsky.app/profile/did:plc:oio4hkxaop4ao4wz2pp3f4cr/post/3lfg76issfc2c">tracking the state</a> of this visibility map, and set up a cron job to do the VACUUM twice a day to keep it relatively &ldquo;clean&rdquo;. (I wish it was as easy to do with my apartment…)</p>
</li>
<li>
<p>Now, it was working great for people with short lists, but still tried to use the <code>(repo, time, id)</code> index for most very long lists. The EXPLAIN showed that it was vastly overestimating the cost of using <code>(time)</code>. I came to the conclusion that… the hardcoded <code>n_distinct</code> was actually working against it now, even though it was technically more precise. In my practical use cases, what actually matters is not how many different users have at least one post anywhere in the table, but how many are <em>actively posting</em> - whose posts you can find in the last few hours or a day. And that&rsquo;s much less than 4.5M. So I cleared the hardcoded <code>n_distinct</code> and let it find its own estimate, and reset the statistics target to the default 100 - and this improved things quite a bit! It was now almost perfectly using <code>index_posts_on_repo_time_id</code> to around ~1200 followed accounts, and <code>index_posts_on_time</code> above that.</p>
</li>
<li>
<p>Not related to the query/Postgres directly, but I also added a &ldquo;prefetching&rdquo; task to the follows list cache: it runs every 15 minutes and checks which follows lists are approaching the point where the cached data becomes stale (but was accessed recently) and reloads the list from the AppView in advance. This saves a couple to several seconds of time every time the list had to be refreshed before returning the feed, so it made quite a lot of difference.</p>
</li>
<li>
<p>One last thing I need to fix is that for long lists, significant time is spent on query planning (e.g. 1724 ms planning time and then 12 ms execution time). I&rsquo;m gonna try to restructure the query to pass the list of user IDs differently somehow and see if that helps, or perhaps I could move the cache to a new database table instead of keeping it in .json files.</p>
</li>
<li>
<p>And finally, I have some more config tweaking to do - e.g. it seems <code>shared_buffers</code> could be bumped up to at least 2 GB here (with 8 GB total RAM) - but I wanted to first make sure it&rsquo;s doing the right things before I give it more space to cache it.</p>
</li>
</ul>
<p>I&rsquo;m really looking forward to when I can finally set up a production instance with Postgres and move everything there and drop the other branches - but I feel like this moment is getting close 😅</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://journal.mackuba.eu/2025/01/08/new-domain/">new domain</a></h1>
		

		<a href="https://journal.mackuba.eu/2025/01/08/new-domain/" class="u-url"><time class="dt-published" datetime="2025-01-08 20:49:13 &#43;0200">Jan 8, 2025</time></a>

		<div class="e-content">
			 <p>who dis</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://journal.mackuba.eu/2025/01/08/test-post/">test post</a></h1>
		

		<a href="https://journal.mackuba.eu/2025/01/08/test-post/" class="u-url"><time class="dt-published" datetime="2025-01-08 17:52:09 &#43;0200">Jan 8, 2025</time></a>

		<div class="e-content">
			 <p>test 123</p>

		</div>
	  </div>
	
	  <div class="h-entry">
		
			<h1><a href="https://journal.mackuba.eu/2025/01/07/hello-world/">Hello world</a></h1>
		

		<a href="https://journal.mackuba.eu/2025/01/07/hello-world/" class="u-url"><time class="dt-published" datetime="2025-01-07 19:55:19 &#43;0200">Jan 7, 2025</time></a>

		<div class="e-content">
			 <p>just setting up my mcroblog</p>

		</div>
	  </div>
	

</div>


    
    

  </body>

</html>
